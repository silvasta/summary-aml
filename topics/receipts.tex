\section{Step-by-Step Instructions for Exam Tasks}

\subsection{"Week 1"}
\begin{enumerate}
  \item \textbf{Classify ML Problem Type:}
    \begin{enumerate}
      \item Identify data: Labeled? (Supervised) Unlabeled? (Unsupervised)
        Rewards? (RL).
      \item Determine goal: Prediction (supervised), patterns (unsupervised),
        decisions (RL).
      \item Example: "Predict house prices from features" $\to$ Supervised
        regression.
    \end{enumerate}

  \item \textbf{Compute Empirical Risk:}
    \begin{enumerate}
      \item Given dataset $\{(x_i, y_i)\}_{i=1}^n$, model $f$, loss $\ell$.
      \item Calculate $\hat{R}(f) = \frac{1}{n} \sum_{i=1}^n \ell(f(x_i),
        y_i)$.
      \item If regularized, add $\lambda \Omega(f)$.
      \item Hint: For MSE, expand to variance + bias terms if asked.
    \end{enumerate}

  \item \textbf{Analyze Bias-Variance:}
    \begin{enumerate}
      \item Decompose error: Compute Bias$^2$ (avg prediction error), Variance
        (spread over datasets).
      \item Suggest fix: High bias $\to$ more features/complex model; high
        variance $\to$ more data/regularization.
      \item Exam trick: Plot learning curves (train vs. test error).
    \end{enumerate}

  \item \textbf{Derive Basic ERM for Linear Regression:}
    \begin{enumerate}
      \item Model: $\hat{y} = w^T x + b$.
      \item Loss: Minimize $\frac{1}{n} \sum (y_i - w^T x_i - b)^2 + \lambda
        \|w\|^2$.
      \item Take derivatives: Set $\nabla_w = 0 \to w = (X^T X + \lambda
        I)^{-1} X^T y$ (closed-form).
      \item Hint: Assume centered data for simplicity.
    \end{enumerate}

  \item \textbf{Identify Overfitting Signs \& Mitigation:}
    \begin{enumerate}
      \item Signs: Low train error, high test error.
      \item Steps: Split data (train/val/test), apply CV, add regularization,
        early stopping.
      \item Conceptual: Explain how it relates to VC dimension (if advanced).
    \end{enumerate}
\end{enumerate}

\emph{General Exam Tip:} Always start with definitions/formulas, then apply to
given scenario. Practice with toy datasets (e.g., 2-3 points) for quick
derivations.

