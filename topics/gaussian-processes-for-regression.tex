\section{Learning Objectives}

- To motivate, understand, and design Gaussian processes.

- To be able to analytically derive procedures
for making predictions with Gaussian processes.

- To analytically compute conditionals, marginals,
and posteriors of Gaussians.

- To formulate and understand kernels.

- To be able to use kernel engineering to design new kernels.

- To be able to make a formal connection between
Gaussian processes and Bayesian linear regression.

\section{Gaussian Processes}

\subsection{Bayesian linear regression}

multiple linear regression model
\[
  Y = X^T \beta + \epsilon
  \quad\text{Gaussian Noise }\epsilon \sim
  \mathcal{N}(\epsilon|0,\sigma^2)
\]
\[
  p(Y|X,\beta,\sigma) =
  \mathcal{N}(Y|X^T\beta,\sigma^2) \propto
  e^{-\frac{1}{2\sigma^2}(Y-X^T \beta)^2}
\]
Bayesian linear regression
extends multiple linear regression
by defining a prior over the regression coefficients,
for example (ridge regression)

% AI: formula for Ridge regression

- Model inversion

% TODO: prepare  formulas for ai

\subsection{Moments of Bayesian linear regression}

Setting

Expected Value

Covariance

% TODO: formulas

\section{Gaussian processes}

Moments of joint Gaussian:

%TODO: moments, rewrite  distribution over y

$Y\sim \mathcal{N}(Y|0,k_{i,j}+\sigma^2 $if $i=j)$

with $k_{i,j}$ kernel function

\textbf{Gaussian Processes as “kernelized linear regression”}

- Kernel functions specify the similarity between any two data points.

\subsection{Recall}

Kernel properties:

- Symmetry

- Positive semi-definit

\subsection{Gram matrix}
Must be positive semi-definit
\[
  K=
  \begin{bmatrix}
    k(x_{1},x_{1})&\cdots&k(x_{1},x_{n})\\
    \vdots&&\vdots\\
    k(x_{n},x_{1})&\cdots&k(x_{n},x_{n})
  \end{bmatrix}
\]
\subsection{Examples of kernel functions}

% AI: math mode for formulas, x0 = x'

Linear kernel:
k(x, x0 ) = xT x0

Polynomial kernel:
k(x, x0 ) = (xT x0 + 1)p , for p ∈ N

Gaussian (RBF) kernel:
k(x, x0 ) = exp −kx − x0 k22 /h2

Sigmoid (tanh) kernel:
k(x, x0 ) = tanh κxT x0 − b

Different kernels have different \textbf{invariance properties}!

For example, invariance to \textbf{rotation} or \textbf{translation.}

\subsection{Kernel engineering by composition}

Addition:
Multiplication:
Scaling:
Composition:

\subsection{Prediction by Gaussian processes}

Predictive density $p(y_{n+1} |x_{n+1} , X, y)$

% HACK: formula with N(Y|0,C_n,k)

Reminder: Conditional Gaussian Distributions

\subsection{Prediction by Gaussian processes}

% WARN: final formula p(Y)

\subsection{Kernel validation}

Goal: Validate hyperparameters of kernels by random splits D

\section{Controller Optimization for Robust Control}

Machine Learning in Control Systems

Machine learning techniques are becoming
more and more important for enabling
computers to control complex and stochastic systems
and predict the outcomes of such systems.

\subsection{Gaussian processes for Control}

\textbf{A Fundamental problem}
when designing controllers for dynamic systems
is the estimation of the controller parameters.
Besides pure statistical performance,
robustness arises as an important design issue.

\textbf{The classical approach}
selects a model of the system to design an
initial controller; parameters are then
tuned manually to achieve best performance.

\textbf{An alternative approach}
uses methods from machine learning to optimize
statistical performance, e.g., Bayesian optimization.

\textbf{Safety-critical system failures}
may happen because these methods evaluate
different controller parameters.

\subsection{Safe optimization}

Overcome safety-critical system failures by using a specialized optimization
algorithm for automatic controller parameter tuning. This algorithm models the
underlying performance measure as a GP and only explores new controller
parameters whose performance lies above a safe performance threshold with high
probability.

