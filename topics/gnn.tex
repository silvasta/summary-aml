\section{Graph Neural Networks (GNNs)}

\subsection{Basics \& Notation}
Graph $G = (V, E)$, $|V| = n$ nodes, adjacency matrix $\mathbf{A} \in
\{0,1\}^{n \times n}$ (symmetric for undirected). Feature matrix $\mathbf{X}
\in \mathbb{R}^{n \times d}$ (node features). Degree matrix $\mathbf{D} =
\diag(\sum_j A_{ij})$.

Normalized adjacency: $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$
(self-loops), $\hat{\mathbf{A}} = \mathbf{D}^{-1/2} \tilde{\mathbf{A}}
\mathbf{D}^{-1/2}$ (symmetric normalization).

Message passing: Update node $v$ as $h_v^{(l+1)} = \sigma \left( \sum_{u \in
\mathcal{N}(v)} m_{u \to v}^{(l)} \right)$, where $m$ aggregates neighbor
info.

\subsection{Graph Convolutional Network (GCN)}
Layer: $\mathbf{H}^{(l+1)} = \sigma(\hat{\mathbf{A}} \mathbf{H}^{(l)}
\mathbf{W}^{(l)})$, with $\mathbf{H}^{(0)} = \mathbf{X}$.

Spectral view: Approximation of graph Laplacian $\mathbf{L} = \mathbf{D} -
\mathbf{A}$, normalized $\hat{\mathbf{L}} = \mathbf{I} - \hat{\mathbf{A}}$.

\subsection{Graph Attention Network (GAT)}
Attention: $\alpha_{ij} = \softmax_j \left( \text{LeakyReLU} \left(
\mathbf{a}^\top [\mathbf{W} h_i \| \mathbf{W} h_j] \right) \right)$

Update: $h_i^{(l+1)} = \sigma \left( \sum_{j \in \mathcal{N}(i) \cup i}
\alpha_{ij} \mathbf{W} h_j^{(l)} \right)$. Multi-head: Concat or average
heads.

\section{Information Theory}

\subsection{Key Measures}
Entropy:
$H(X) = -\mathbb{E}_{p(x)} [\log p(x)]$

Joint entropy: $H(X,Y) = -\mathbb{E}[\log p(x,y)]$.

Conditional: $H(Y|X) = H(X,Y) - H(X)$.

Mutual information: $I(X;Y) = H(X) + H(Y) - H(X,Y) = H(X) - H(X|Y) =
\KL(p(x,y) \| p(x)p(y)) \geq 0$.

Cross-entropy: $H(p,q) = -\mathbb{E}_p [\log q] = H(p) + \KL(p \| q)$.

\textbf{KL divergence} $\KL(p \| q) = \mathbb{E}_p [\log (p/q)] \geq 0$

Tricks: Jensen-Shannon divergence for stability:
$\text{JSD}(p \| q) = \frac{1}{2} \KL(p \| m) + \frac{1}{2} \KL(q \|
m)$, $m=(p+q)/2$.

