\section{SVM and convex optimization}

\subsection{Learning objectives}

- Lagrange multipliers

- Basics of convex optimization and duality

\subsection{Instability of the perceptron}

\subsection{Maximum-margin criterion}

\subsection{Lagrange Multipliers}

\subsection{Convex optimization}
\subsubsection{The dual}

\subsubsection{Weak duality and strong duality}

\subsubsection{Slater’s condition}

\subsubsection{Consequences of strong duality}

− The primal optimal solution is the minimizer of the Lagrangian:

− Complementary slackness:

\subsubsection{Convex optimization via the dual}

% TODO: for SVMs...

\subsection{SVMs}

% HACK: derivation SVM

- Formula seperation band

- Projection

- Normalization trick

- Formalizatin of primal

Why do we compute the dual?

- SVM calculation can become intractable in high dimensions
or when working with transformations (more on this later).

- The dual does not depend on the dimensionality
of the dataset in the primal!

Complementary slackness

What about the intercept?

What if the dataset is not linearly separable?

- Soft-margin SVMs

- Kernels

\subsection{Soft-margin SVMs}

- Formulation

- The role of C

dual ...

\section{Kernelization}

Transformations and kernels

Polynomial transformations

SVMs and kernels

\subsection{Kernels}

no need for $\phi$
only $\phi^T(x)\phi(x')$
for some cases of x and x′

% TODO: table kernel
%
\section{SVM summary}

% NOTE: combine everything in the end, 1 structure

Support Vector Machine (SVM): Idea

- margin

- kernel

Nonlinear Transformation in Kernel Space

SVM Lagrangian for functional margin formulation

% HACK: formula w5_s.4/27 SVM

Non-separable case: Soft Margin SVM

Learning the Soft Margin SVM

\section{Structural SVMs}

From margins to score functions

Extensions to the SVM
% TODO: extensions

Multiclass SVMs (linear discriminant function)

% TODO: formulation s10

Structural SVMs

% WARN: parser w5_s.s12/27

..more examples
