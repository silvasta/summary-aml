
% TODO: nice graphic

\section{Tokenization and embeddings}

- Topic classification

Strategy for NLP

We create for each word in the sentence an embedding vector.
Then we aggregate these embeddings
to create an embedding for the sentence.
We then apply a function on the embedding to
map the sentence to a distribution of topics.

- Tokenization

The first step is to
transform the sentence into a sequence of tokens.

A popular method is the WordPiece tokenization.
It produces a decomposition of a training corpus of text into
$S$ tokens, where $S$ is predefined in advance.

1. Set T = set of characters occurring in text

2. While size of T > S do:

1. Find the most common pair a, b of tokens in T occurring in the text.

2. Remove a and b from T and put instead a new token ab in T.

\section{Self-attention}

- Sentiment classification for movies

- Attention

- Relational matrices

These maps can also be used to infer meanings from words

% NOTE: example bat (schl√§ger,fledermaus)

Every word finds its meaning through other words in the sentence!

Attention as information retrieval

An attention map for location of objects

- Query

- Keys

- Values

% TODO: check again example graphics

Abstraction of an attention mechanism

% NOTE: formula/diagramm derivation

% HACK: final diagramm Attention

\subsection{Multi-headed attention}

% TODO: diagramm Multi-headed attention

\subsection{Cross-attention}

Consider the problem of translating from English to German:

% NOTE: example?

- A different attention mechanism is needed

Abstraction of a cross-attention mechanism

\subsection{Masked self-attention}

Consider the task of generating text for answering questions.

% TODO: diagramm masked self-attention

\section{Positional encodings}

- Consider the sentences
the dog chased the cat and the cat chased the dog.

- They are permutations of each other,
so the attention maps will also
just be permutations of each other.

- Note that attention mechanisms do not pay attention
to the position of a token in the sentence.

- Binary positional encoding

- Positional encodings

Why adding and not contatenating?

\section{Broadcasting}

- Broadcasting in PyTorch

- Examples of broadcasting

Broadcasting formalized

% TODO: formulation w6_t.83/93

Implementing the masked-attention mechanism

\section{Residual Blocks}

- The degradation problem

- Residual blocks

\section{Transformer Architecture}

% TODO: image

BERT: Bidirectional Encoder Representations from Transformers

