\section{(PO-)MDP by construction}

A (partially observable) Markov decision process is a tuple
\[
  (\mathcal{S}, \mathcal{A}, p, r, p_0, \gamma)
\]

- $\mathcal{S}$ set of states (measurable space if continuous)

- $\mathcal{A}$ action set (discrete or continuous)

- $p(\cdot|s,a)$ transition kernel over $\mathcal{S}$

- $r(s,a,s') \in \mathbb{R}$ reward

- $p_0$ initial-state distribution

- $\gamma \in (0,]$ discount?

\textbf{Observations}

In a POMDP, agent perceives
$o \sim \mathcal{O}(\cdot|s)$

In a MDP, $o=s$
(full observability)

\subsection{Policies}

A policy is a function $\pi$ that maps each state $s\in S$
to a distribution over the action set $\pi(\cdot|s)$.

We distinguish between two types of policies:

- Deterministic: $\mu : S \to A$, when $\pi(\cdot|s)$ has no randomness.

- Stochastic: $\pi(\cdot|s)$, otherwise.

\subsection{Exploration–exploitation}

Goal: maximize expected return while reducing uncertainty.

- $\epsilon$-greedy:
with probability $\epsilon$ pick random action,
else arg $\max_a Q$

- Softmax/Boltzmann:
$\pi(a | s) \propto e^{Q(s,a)/\tau}$
with temperature $\tau > 0$

- Anneal $\epsilon$ or $\tau$ over time.

\subsection{Trajectories}

Given a policy $\pi$, a trajectory is:
\[
  \tau = (s_0,a_0,r_0,s_1,a_1,r_1,\dots)
\]

% NOTE: definition MDP trajectory

\subsection{Markov property}

By construction:
\[
  \mathbb{P}(s_{t+1}|s_{0:t},a_{0:t}) =
  \mathbb{P}(s_{t+1}|s_t,a_t)
\]
Consequences:

- Sufficient statistic is current $(s_t,a_t)$

- Enables dynamic programming / Bellman recursion.

\subsection{Value functions}

For policy $\pi$:

$V^\pi(s) = \mathbb{E}_\pi[G|s]$

$Q^\pi(s,a) = \mathbb{E}_\pi[G|s,a]$

Relation for discrete $\mathcal{A}$:

$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$

\subsection{Bellman equations}

Policy evaluation:

% TODO: Bellman

Optimality (MDP):

\subsection{Q-learning (tabular, off-policy)}

Update at $(s_t, a_t, r_t, s_{t+1})$:
\[
  Q \leftarrow Q + \alpha
  [r_t + \gamma\max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)]
\]

\subsection{DQN (function approximation)}

% AI: explain the following

- Approximate $Q_\theta(s,a)$ with an MLP.

- Target network $\theta^-$ ; replay buffer for decorrelation.

- Squared TD error with max over $a′$ using $\theta^-$

\section{Active learning}

% TODO: Formalization active learning

\subsection{Information-based transductive learning}

% TODO: Information-based transductive learning (Hübotter et al, 2024)

\subsection{Safe Bayesian optimization}

We want to find the maximum of an unknown stochastic process $f^\star$

We can iteratively choose points $x_1,\dots,x_{n-1} \in X$
and observe realizations of $y_i = f^\star(x_i)$

We must not pick points outside the safe area
$S^\star = \{x\in X: g^\star(x)\ge 0\}$,
where $g^\star$ is another stochastic process.

We also observe the realizations  $z_i = g^\star(x_i)$

How do we pick $x_n$ so that we can estimate best where the
maximum in expectation of $f^\star$ is?

\subsubsection{ Bayesian optimization with ITL}

Fit a GPs $f$ on $\{x_i,y_i\}$ for $i<n$

This GP induces 2 functions, $l_n^f$ and $u_n^f$, defined by requirement:

- $\forall x\in X, [l_n^f(x), u_n^f(x)]$ is 95\%-confidence interval
of $\mathbb{E}[f(x)]$

Similarly, fit a GP $g$ on $\{x_i,z_i\}$ for $i<n$

This GP induces 2 functions, $l_n^g$ and $u_n^g$ in an analogous way
and they in turn induce the following pessimistic and optimistic
estimates of the safe set
\[
  \mathcal{S}_n =\{x:l_n^g(x)\ge0\}
  \text{ and }
  \hat{\mathcal{S}_n} =\{x:u_n^g(x)\ge0\}
\]
We can then consider the following set of estimates
\[\mathcal{A}_n = \{
    x\in\hat{\mathcal{S}_n}:
    u_n^f(x)\ge
    \max_{x'\in\mathcal{S}_n}
    l_n^f(x')
\}\]

We can then do ITL using as
sample space $\mathcal{S}_n$ and
target space $\mathcal{A}_n$

\subsection{Batch active learning}

We assume given the following:

− an input domain $\mathcal{X}$ and a distribution $P$ over $\mathcal{X}$

− an oracle to an unknown function $f:\mathcal{X}\to\mathcal{Y}$

− a population set $X = \{x_i,\dots,x_m\}\subseteq \mathcal{X}$

− a budget $b\le m$

Our goal is to find $L\subseteq X$ such that $|L| = b$ and query the
oracle for $\{f(x):x\in L\}$.

% TODO: AUXILIARY DEFINITIONS
AUXILIARY DEFINITIONS

\subsubsection{Problem formalization}

% TODO: Batch active Learning, solution strategy and algorithm

