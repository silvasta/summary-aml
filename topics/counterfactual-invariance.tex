

\section{Reproducing Kernel Hilbert Spaces (RKHS)}

A Reproducing Kernel Hilbert Space (RKHS) is a Hilbert space
$\mathcal{H}$ of functions $f: \mathcal{X} \to \mathbb{R}$ with a
kernel $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ such that:
\begin{itemize}
  \item $k$ is positive semi-definite (PSD): For any $x_i$, the Gram
    matrix $K_{ij} = k(x_i, x_j) \succeq 0$.
  \item Reproducing $\langle f, k(x, \cdot)
    \rangle_{\mathcal{H}} = f(x)\ \forall f \in \mathcal{H}$. %
\end{itemize}

\textbf{Counterfactual invariance} In causal ML, models invariant under
interventions (e.g., do-calculus). For a structural causal model
(SCM) $Y = f(X, U)$, counterfactuals ask "What if?" (e.g., $Y_{x'}$
where $x'$ is intervened). Invariance ensures predictions stable
across envs.

\textbf{Moore-Aronszajn} Every PSD kernel $k$ defines a unique RKHS where
span$\{k(x,\cdot)\}$ is dense.

\textbf{Mercer} for continuous PSD kernels on compact
$\mathcal{X}$, $k(x,x') = \sum_{i=1}^\infty \lambda_i \phi_i(x)
\phi_i(x')$, with $\lambda_i \geq 0$, enabling eigen-decomposition.

\textbf{SVMs} K decision $f(x) = \sum \alpha_i y_i
k(x_i, x) + b$\\
\textbf{GP} Prior $f \sim \mathcal{GP}(m,k)$ in RKHS,\\
posterior mean $\bar{f}(x_*) = k_*^\top (K + \sigma^2 I)^{-1} y$\\
Counterfactuals in ML: Use invariant risk minimization (IRM) to
minimize risk invariant to spurious correlations (e.g., Arjovsky et
al.). % Addition: Tied to lectures on causal invariance.

\textbf{Solve}
Use reproducing property for f evaluation:
$f(x) = \langle f, k(x,\cdot) \rangle$
Show PSD via Mercer.

\textbf{Counterfactual Trick}: For invariance, compute
$P(Y|do(X=x'))$ using causal graphs; compare to observational $P(Y|X)$.

