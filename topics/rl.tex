
\section{RL \& Active Learning}

\subsection{Markov Decision Processes (MDPs)}

$V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty
\gamma^t R(s_t, a_t) \mid s_0 = s \right]$

\textbf{Action-value}
$Q^\pi(s,a) = \mathbb{E}_\pi \left[
\sum_{t=0}^\infty \gamma^t R(s_t, a_t) \mid s_0=s, a_0=a \right]$

\textbf{Advantage} $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$.

\textbf{Bellman Optimality:} $V^*(s) = \max_a \sum_{s',r} P(s',r|s,a) [r +
\gamma V^*(s')]$

\textbf{Discounted Return:} $G_t = \sum_{k=t}^\infty \gamma^{k-t} R_{k+1}$.

\textbf{Exploration}
$\epsilon$-greedy (random w.p. $\epsilon$), UCB
($a = \arg\max [Q(s,a) + c \sqrt{\frac{\ln t}{N(s,a)}}]$)

\textbf{Policy Gradient Thm:} $\nabla_\theta J(\theta) = \mathbb{E}_\pi
[\nabla_\theta \log \pi_\theta(a|s) Q^\pi(s,a)]$

\textbf{REINFORCE:}
$ \hat{\nabla} J = \sum_t \nabla_\theta \log \pi(a_t|s_t) G_t$.
Var. reduct. Subtract baseline $b(s_t) \approx
V(s_t)$

\textbf{Actor-Critic:}
$A = r + \gamma V(s') - V(s)$.

