
\textbf{Sigm.}
$\sigma(x) = \frac{1}{1 + e^{-x}}$, $\sigma'(x) = \sigma(x)(1 - \sigma(x))$
\\
\textbf{Variance}
$\mathbb{E}[(\hat{y} - \mathbb{E}[\hat{y}])^2]$

\subsection*{Loss Functions}

\textbf{Logistic} $\ell(y, p) = -y \log p - (1-y) \log(1-p)$
\\
\textbf{Cross-Entropy loss} $\mathcal{L} = -\sum_i y_i \log(\hat{y}_i)$

\section{Representations}

\subsection{Empirical Risk Minimization (ERM)}
$R(f) = \mathbb{E}_{(x,y) \sim \mathcal{D}} [\ell(f(x), y)]$
$\mathcal{D}$ data distrib. \\
Empirical Risk:
$\hat{R}(f) = \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)$

\textbf{CRLB}
Lower bound on variance of unbiased estimators $\hat{\theta}$
of param $\theta$. Assumes regularity: differentiable
log-likelihood, finite variance.
\\
\textbf{Trick}
CRLB achieved iff estimator is efficient,
Multivariate: Use inverse Fisher matrix.
\\
\textbf{Hint for exams} Always check unbiasedness first; compute via Hessian or
score function.

\subsection{Formulas: Fisher Information}

$\mathbb{E}\left[ \left( \frac{\partial}{\partial \theta} \log
p(X|\theta) \right)^2 \right] = -\mathbb{E}\left[ \frac{\partial^2}{\partial
\theta^2} \log p(X|\theta) \right]$

Multivariate ($\theta \in \mathbb{R}^k$): Matrix form

$[I(\theta)]_{ij} = \mathbb{E}\left[ \frac{\partial \log p}{\partial \theta_i}
\frac{\partial \log p}{\partial \theta_j} \right] = -\mathbb{E}\left[
  \frac{\partial^2 \log p}{\partial \theta_i \partial \theta_j}
\right]$

Trick: For iid  $X_1,..,X_n$, $I_n(\theta) = n I(\theta)$

Example: Gaussian $\mathcal{N}(\mu, \sigma^2=1)$
\\
Score: $\frac{\partial \log p}{\partial \mu} = x - \mu$. Fisher: $I(\mu) = 1$.

\subsection{Rao-Cram√©r Lower Bound (CRLB)}
For unbiased $\hat{\theta}(X)$:
$\Var(\hat{\theta}) \geq \frac{1}{I(\theta)} \quad (\text{scalar})$

Multiv.
$\Var(g(\hat{\theta}))
\geq \left( \frac{\partial g}{\partial \theta} \right)^T I(\theta)^{-1}
\left( \frac{\partial g}{\partial \theta} \right)$

General CRLB: $\Cov(\hat{\theta}) \succeq I(\theta)^{-1}$

Equality if $\hat{\theta} = a(\theta) \cdot s(X) + b(\theta)$,
$s(X)$ is suff. stat. (Gauss. sample mean givs CRLB)

\subsection{Calculus Recipes \& Derivations}
\begin{itemize}
  \item Compute $I(\theta)$: (1) Write $\log L(\theta|X) = \sum \log
    p(x_i|\theta)$. (2) Take 2nd deriv or score sq. (3) Expectation over
    $p(X|\theta)$.
  \item For representations: Info in feature space: $I_\phi(\theta) =
    \mathbb{E}[\phi(X)^T \phi(X)]^{-1}$
  \item Exam hint: CRLB bounds learning rates (e.g., variance in param
    est. for neural nets).
\end{itemize}

