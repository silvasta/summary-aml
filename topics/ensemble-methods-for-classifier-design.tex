\section{Model averaging is common practice}

- Previous: Gaussian process motivated by Bayesian linear regression.

- Seldom: take MAP estimator in Bayesian setting.

- Bayesian approach: average models with different parameters
(weighted according to prior).

- Cross validation: Take average over models trained on different folds.

- Winners of most Machine Learning competitions (e.g. on Kaggle):
ensembles (weighted averages of models).

-
\section{Combining Regressors - Bias}

TODO: formula

-
\section{Combining Regressors - Variance}

TODO: formula

\section{Ensemble Learning}

\textbf{The idea of classifier ensembles}
Boosting is an approach to machine learning
based on the idea of creating
a highly accurate prediction rule
by combining many relatively weak and inaccurate rules.

- Computational advantage
% AI: short explanation

- Statistical advantage
% AI: short explanation

\section{Induction Principles for Classifier Selection}

I) Empirical Risk Minimization (ERM) Principle
% AI: short explanation

II) Bayesian inference by model averaging
% AI: short explanation

\section{Motivation for Ensemble Methods}

% AI: 2 sentences per: - point

- Train several sufficiently diverse predictors

- Bagging

- Arcing

- Boosting

% AI: explain relevance of the above, length doesn't mater

\section{Weak Learners Used for Bagging or Boosting}

Combining Classifiers

Bagging Classifiers

Classifier selection: First compare, then bag!

Bagging: The Mechanism

Decision Trees

Random Forests

The Idea of Boosting

AdaBoost

Data Reweighting

Boosted Classifier

Comparison of ensemble methods

\section{Loss functions for classification}

% TODO: examples

% TODO: check special slides (2019?) random forest

