

\section{Neural Networks: Basics}

$\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} +
\mathbf{b}^{(l)}$, $\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})$

\textbf{Sigm.}
$\sigma(x) = \frac{1}{1 + e^{-x}}$, $\sigma'(x) = \sigma(x)(1 - \sigma(x))$

\textbf{Cross-Entropy loss} $\mathcal{L} = -\sum_i y_i \log(\hat{y}_i)$

\textbf{Backpropagation}:
Output gradient:
$\delta^{(L)} = \frac{\partial \mathcal{L}}{\partial
\mathbf{z}^{(L)}} = (\hat{\mathbf{y}} - \mathbf{y}) \odot
\sigma'(\mathbf{z}^{(L)})$
\\
Hidden:
$\delta^{(l)} = (\mathbf{W}^{(l+1)T} \delta^{(l+1)}) \odot
\sigma'(\mathbf{z}^{(l)})$

Weight update: $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} =
\delta^{(l)} \mathbf{a}^{(l-1)T}$.
\textit{Trick}: Use chain rule; initialize
weights $\sim \mathcal{N}(0, \frac{2}{n_{in}})$

\textbf{Optimization Tricks}: Gradient Descent: $\theta \leftarrow \theta -
\eta \nabla \mathcal{L}$. Momentum: Add velocity term. Adam: Adaptive learning
rates with moments.

\section{Attention Mechanisms}
\textbf{Scaled Dot-Product Attention}
$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
= \softmax\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V}$
\\
$\mathbf{Q}$: Queries ($n \times d_k$),
$\mathbf{K}$: Keys ($m \times d_k$),
$\mathbf{V}$: Values ($m \times d_v$)
- \textit{Trick}: Scaling prevents softmax saturation; causal mask for
decoders (upper triangle $-\infty$).

\textbf{Multi-Head Attention}
Concat($\text{head}_1,.., \text{head}_h) \mathbf{W}^O$, each head:
$\text{head}_i = \text{Attention}(\mathbf{Q} \mathbf{W}_i^Q,
\mathbf{K} \mathbf{W}_i^K, \mathbf{V} \mathbf{W}_i^V)$,
$h=8$ typical, allow parallel focus on subspaces

\textbf{Self-Attention}:
$\mathbf{Q} = \mathbf{K} = \mathbf{V} = \mathbf{X} \mathbf{W}$
\\
(input projection). \textit{Exam Tip}: Captures dependencies
without recurrence; $O(n^2)$ time.

\section{Transformers}
\textbf{Architecture}: Encoder (self-attn + FFN) stack; Decoder (masked
self-attn + enc-dec attn + FFN) stack.
- FFN: Two linear layers with ReLU: $\text{FFN}(\mathbf{x}) = \max(0,
\mathbf{x} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2$.
- Residual: $\mathbf{x} \leftarrow \mathbf{x} + \text{Sublayer}(\mathbf{x})$.
LayerNorm after.

\textbf{Positional Encoding}
Add to input embeddings.

$\text{PE}_{(pos, 2i{\color{RoyalBlue}|+1})}
= \sin{\color{RoyalBlue}|\cos}\left(
\frac{pos}{10000^{2i/d}} \right)$

Allows order awareness; fixed or learned.

