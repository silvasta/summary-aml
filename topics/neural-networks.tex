

\section{Neural Networks: Basics}

\subsection{Propagation}
Forward: \(z^l = W^l a^{l-1} + b^l, a^l = \sigma(z^l)\)
\\
Backward: \(\delta^L = \nabla_a L \odot \sigma'(z^L), \delta^l =
(W^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)\)
\\
Weight update: \(\frac{\partial L}{\partial W^l} = \delta^l (a^{l-1})^T\)

\section{Attention Mechanisms}

\(\text{Attention}(Q, K, V) =
\softmax\left(\frac{QK^T}{\sqrt{d_k}}\right) V\)
\\
Q/K/V: Linear projections of input.
Scaled for stability (prevents large dot-products).

\subsection{Multi-Head Attention}
$\text{Concat}(\text{head}_1,.., \text{head}_h) W^O$,
$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$
- h heads project to subspaces (e.g., h=8).
- Advantage: Captures multiple dependency types.

\section{Transformers}
\textbf{Architecture}: Encoder (self-attn + FFN) stack; Decoder (masked
self-attn + enc-dec attn + FFN) stack.
- FFN: Two linear layers with ReLU: $\text{FFN}(\mathbf{x}) = \max(0,
\mathbf{x} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2$.
- Residual: $\mathbf{x} \leftarrow \mathbf{x} + \text{Sublayer}(\mathbf{x})$.
LayerNorm after.

\textbf{Positional Encoding}
Add to input embeddings.

$\text{PE}_{(pos, 2i{\color{RoyalBlue}|+1})}
= \sin{\color{RoyalBlue}|\cos}\left(
\frac{pos}{10000^{2i/d}} \right)$

Allows order awareness; fixed or learned.

