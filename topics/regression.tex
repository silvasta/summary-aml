\section{Linear Regression}

- Statistical model

\[
	Y=X^{\mathsf{T}}\beta.\ \ \ \ Y\in\mathbb{R}.\ X.\beta\in\mathbb{R}^{d+1}
\]
- Residual Sum of Squares (RSS)

\[
	R S S(\beta)\;=\;({\bf y}-{\bf X}\beta)^{\mathsf{T}}({\mathbf y}-{\mathbf X}\beta)
\]
\[
	{\hat{\boldsymbol{\beta}}}=(\mathbf{X}^{\mathsf{T}}\mathbf{X})^{-1}\mathbf{X}^{\mathsf{T}}\mathbf{y}
\]

% AI: write above cleaner

\section{Gauss Markov Theorem}

% AI: show and prove (short)

\section{Bias/Variance Dilemma}

% TODO: setup, neccesssairy?

- Tradeoff, split Error

- Identify error components

\section{Bayesian Maximum A Posteriori (MAP) estimates}

\subsection{Ridge Regression}

% AI: formulas to all points 
- Cost function

- Bayesian view

- Solution

%AI: formula, very short  explanation
Tikhonov regularization

\subsection{LASSO}

% AI: formulas to all points 
- Cost function

- Bayesian view

- Solution

\subsection{Ridge vs. LASSO Estimation}

%AI: short comparison

\section{Remarks on Shrinkage Methods}

- Generalized Ridge Regression

% TODO: formula

\textbf{Idea behind shrinkage}
When white noise is added to the data then all Fourier
coefficients are increased by a constant on average.
â‡’ Shrink all coefficients by the estimated noise
amount to derive a robust predictor.
